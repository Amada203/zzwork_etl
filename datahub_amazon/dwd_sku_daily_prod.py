#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
dwd_sku_daily æ—¥åº¦SKUè¡¨ETLè„šæœ¬
ä» datahub_amazon.dwd_sku_info ç”Ÿæˆæ—¥åº¦æ±‡æ€»è¡¨
æ¯ä¸ªå­—æ®µåˆ†åˆ«å–å½“å¤©éç©ºénullï¼Œetl_sourceä¼˜å…ˆï¼Œsnapshot timeæœ€æ™šçš„å€¼
"""

import logging
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, collect_list, when, col
from pyspark.sql.types import StringType, ArrayType
from pigeon.connector import new_impala_connector
from pigeon.utils import init_logging
from dw_util.util import str_utils
# from spark_util.insert import spark_write_hive  # æœªä½¿ç”¨

# ==================== é…ç½®å‚æ•° ====================
TARGET_DB = 'datahub_amazon'
TARGET_TABLE = 'dwd_sku_daily'
CALC_PARTITION = '{{ yesterday_ds }}'
REFRESH_STATS = False
EMR = True
PARTITION_NUM = 400
COMPRESSION = 'snappy'
TEST_LIMIT = None  # ç”Ÿäº§ç¯å¢ƒä¸ä½¿ç”¨é™åˆ¶
UPSTREAM_TABLE = 'datahub_amazon.dwd_sku_info'
FEISHU_URL = 'https://yimiandata.feishu.cn/wiki/E81Zw4jK7iJYbGkh5Ejcc3vjnmh?sheet=h2Kd9N'


DDL = f"""
CREATE TABLE IF NOT EXISTS {TARGET_DB}.{TARGET_TABLE} (
    product_id STRING,
    sku_id STRING,
    product_title STRING,
    url STRING,
    color STRING,
    size STRING,
    brand STRING,
    brand_id BIGINT,
    std_brand_name STRING,
    manufacturer STRING,
    has_sku INT,
    variant_information STRING,
    category STRING,
    sub_category STRING,
    category_id STRING,
    category_name STRING,
    category_1_id INT,
    category_1_name STRING,
    category_2_id INT,
    category_2_name STRING,
    category_3_id INT,
    category_3_name STRING,
    category_4_id INT,
    category_4_name STRING,
    category_5_id INT,
    category_5_name STRING,
    category_6_id INT,
    category_6_name STRING,
    category_7_id INT,
    category_7_name STRING,
    category_8_id INT,
    category_8_name STRING,
    category_9_id INT,
    category_9_name STRING,
    seller STRING,
    seller_id STRING,
    first_image STRING,
    imags STRING,
    video STRING,
    specifications STRING,
    additional_description STRING,
    extra_json STRING,
    etl_source STRING,
    snapshot_time STRING
) PARTITIONED BY (
    region STRING,
    dt STRING
) STORED AS PARQUET
"""

# QUERY_COLUMNS = str_utils.get_query_column_from_ddl(DDL)  # æœªä½¿ç”¨
# PARTITION_DICT = None  # æœªä½¿ç”¨

# ==================== é£ä¹¦æ–‡æ¡£è·å–å‡½æ•° ====================
def get_etl_source_priority():
    """ä»é£ä¹¦æ–‡æ¡£è·å–etl_sourceä¼˜å…ˆçº§æ’åº"""
    from pigeon.connector.feishu import FeishuBot
    
    bot = FeishuBot()
    token = FEISHU_URL.split('/wiki/')[1].split('?')[0]
    sheet = FEISHU_URL.split('sheet=')[1].split('&')[0]
    df = bot.read_feishusheet(file_token=token, sheet=sheet)
    
    etl_source_priority = df['etl_source'].tolist()
    logging.info(f"è·å–åˆ°etl_sourceä¼˜å…ˆçº§: {len(etl_source_priority)}ä¸ª")
    return etl_source_priority

def build_priority_case_statement(etl_source_priority):
    """æ„å»ºetl_sourceä¼˜å…ˆçº§æ’åºçš„CASEè¯­å¥"""
    case_parts = [f"WHEN '{source}' THEN {i}" for i, source in enumerate(etl_source_priority, 1)]
    return f"CASE etl_source {' '.join(case_parts)} ELSE {len(etl_source_priority) + 1} END"

# ==================== å·¥å…·å‡½æ•° ====================
def execute_sql(sql, executor):
    """æ‰§è¡ŒSQLæŸ¥è¯¢"""
    return executor(sql)

def merge_json_udf(json_list):
    """
    é«˜æ•ˆçš„JSONåˆå¹¶UDF
    ç›¸åŒkeyå–æœ€ä¼˜ï¼Œä¸åŒkeyåˆå¹¶
    """
    if not json_list or len(json_list) == 0:
        return '{}'
    
    merged_dict = {}
    
    for json_str in json_list:
        if not json_str or json_str.strip() == '' or json_str.strip() == '{}':
            continue
            
        try:
            data = json.loads(json_str)
            if isinstance(data, dict):
                # åˆå¹¶å­—å…¸ï¼šç›¸åŒkeyå–æœ€ä¼˜ï¼ˆåé¢çš„è¦†ç›–å‰é¢çš„ï¼‰
                merged_dict.update(data)
        except (json.JSONDecodeError, TypeError):
            # å¿½ç•¥æ— æ•ˆçš„JSON
            continue
    
    return json.dumps(merged_dict, ensure_ascii=False)

# æ³¨å†ŒUDF
merge_json_udf_func = udf(merge_json_udf, StringType())

# ==================== æ•°æ®å¤„ç†å‡½æ•° ====================
# ç§»é™¤åŠ¨æ€SQLç”Ÿæˆå‡½æ•°ï¼Œç›´æ¥ä½¿ç”¨é™æ€SQL

def dumper_daily_sku(spark, calc_partition):
    """å¤„ç†dwd_sku_info -> dwd_sku_daily æ—¥åº¦è¡¨"""
    limit_clause = f"LIMIT {TEST_LIMIT}" if TEST_LIMIT else ""
    
    # å…ˆæ£€æŸ¥ä¸Šæ¸¸è¡¨æ•°æ®é‡
    check_sql = f"""
    SELECT COUNT(*) as total_count
    FROM {UPSTREAM_TABLE}
    WHERE dt = '{calc_partition}'
      AND region IS NOT NULL AND region != ''
    """
    try:
        result = spark.sql(check_sql).collect()
        total_count = result[0]['total_count'] if result else 0
        print(f"ğŸ“Š ä¸Šæ¸¸è¡¨ {UPSTREAM_TABLE} åœ¨ {calc_partition} çš„æ•°æ®é‡: {total_count}")
        
        if total_count == 0:
            print(f"âš ï¸  è­¦å‘Š: ä¸Šæ¸¸è¡¨åœ¨ {calc_partition} æ²¡æœ‰æ•°æ®ï¼ŒETLå°†è¿”å›ç©ºç»“æœ")
            # è¿”å›ç©ºçš„DataFrame
            empty_schema = spark.sql(f"SELECT * FROM {UPSTREAM_TABLE} LIMIT 0").schema
            return spark.createDataFrame([], empty_schema)
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ä¸Šæ¸¸è¡¨æ•°æ®é‡å¤±è´¥: {e}")
    
    # è·å–etl_sourceä¼˜å…ˆçº§æ’åº
    etl_source_priority = get_etl_source_priority()
    priority_case_statement = build_priority_case_statement(etl_source_priority)
    
    # å®šä¹‰å˜é‡é¿å…f-stringè¯­æ³•é”™è¯¯
    empty_json = '{}'
    comma_sep = ','
    
    base_query = f"""
    WITH source_data AS (
        SELECT *,
        FIRST_VALUE(if(sku_id IS NULL OR sku_id = '', NULL, sku_id)) IGNORE NULLS OVER (
                PARTITION BY  region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as optimized_sku_id
        FROM {UPSTREAM_TABLE}
        WHERE dt = '{calc_partition}'
          AND region IS NOT NULL AND region != ''
        {limit_clause}
    ),
    
    -- æ”¶é›†æ‰€æœ‰éç©ºä¸”ä¸é‡å¤çš„extra_json
    extra_json_collected AS (
        SELECT 
            optimized_sku_id,
            region,
            dt,
            COLLECT_SET(
                CASE 
                    WHEN extra_json IS NOT NULL AND extra_json NOT IN ('', '{empty_json}')
                    THEN extra_json
                    ELSE NULL
                END
            ) as extra_json_list
        FROM source_data
        GROUP BY optimized_sku_id, region, dt
    ),
    
    
    
    
    -- å¯¹æ¯ä¸ªå­—æ®µåˆ†åˆ«å–æœ€ä¼˜å€¼ï¼šå½“å¤©éç©ºénullï¼Œetl_sourceä¼˜å…ˆï¼Œsnapshot timeæœ€æ™š
    field_optimized AS (
        SELECT 
            optimized_sku_id as sku_id,
            region,
            dt,
            
            FIRST_VALUE(if(product_id IS NULL OR product_id = '', NULL, product_id)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as product_id,
            
            FIRST_VALUE(if(product_title IS NULL OR product_title = '', NULL, product_title)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as product_title,
            
            FIRST_VALUE(if(url IS NULL OR url = '', NULL, url)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as url,
            
            FIRST_VALUE(if(color IS NULL OR color = '', NULL, color)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as color,
            
            FIRST_VALUE(if(size IS NULL OR size = '', NULL, size)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as size,
            
            FIRST_VALUE(if(brand IS NULL OR brand = '', NULL, brand)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as brand,
            
            FIRST_VALUE(if(manufacturer IS NULL OR manufacturer = '', NULL, manufacturer)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as manufacturer,
            
            FIRST_VALUE(if(has_sku IS NULL, NULL, has_sku)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as has_sku,
            
            FIRST_VALUE(if(variant_information IS NULL OR variant_information = '', NULL, variant_information)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as variant_information,
            
            FIRST_VALUE(if(category IS NULL OR category = '', NULL, category)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as category,
            
            FIRST_VALUE(if(sub_category IS NULL OR sub_category = '', NULL, sub_category)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as sub_category,
            
            FIRST_VALUE(if(seller IS NULL OR seller = '', NULL, seller)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as seller,
            
            FIRST_VALUE(if(seller_id IS NULL OR seller_id = '', NULL, seller_id)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as seller_id,
            
            FIRST_VALUE(if(first_image IS NULL OR first_image = '', NULL, first_image)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as first_image,
            
            FIRST_VALUE(if(imags IS NULL OR imags = '', NULL, imags)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as imags,
            
            FIRST_VALUE(if(video IS NULL OR video = '', NULL, video)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as video,
            
            FIRST_VALUE(if(specifications IS NULL OR specifications = '', NULL, specifications)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as specifications,
            
            FIRST_VALUE(if(additional_description IS NULL OR additional_description = '', NULL, additional_description)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as additional_description,
            
            -- extra_json å¤„ç†ï¼šå–ä¼˜å…ˆçº§æœ€é«˜çš„éç©ºå€¼
            FIRST_VALUE(if(extra_json IS NOT NULL AND extra_json != '' AND extra_json != '{empty_json}', extra_json, NULL)) IGNORE NULLS OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as extra_json,
            
            CAST(NULL AS STRING) as etl_source,
            
            FIRST_VALUE(snapshot_time) OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as snapshot_time,
            
            -- æ·»åŠ è¡Œå·ç”¨äºå»é‡
            ROW_NUMBER() OVER (
                PARTITION BY optimized_sku_id, region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as rn
        FROM source_data
    ),
    
    -- å»é‡ï¼šåªä¿ç•™æ¯ä¸ªåˆ†åŒºçš„ç¬¬ä¸€æ¡è®°å½•
    deduplicated AS (
        SELECT 
            sku_id, region, dt, product_id, product_title, url, color, size, brand,
            manufacturer, has_sku, variant_information, category, sub_category,
            seller, seller_id, first_image, imags, video, specifications, 
            additional_description, extra_json, etl_source, snapshot_time
        FROM field_optimized
        WHERE rn = 1
    ),
    
    -- æœ€ç»ˆç»“æœï¼šå»é‡å¹¶æ·»åŠ é¢„ç•™å­—æ®µï¼Œé›†æˆå»é‡åçš„extra_json
    final_result AS (
        SELECT 
            d.product_id, d.sku_id, d.product_title, d.url, d.color, d.size, d.brand, 
            CAST(NULL AS BIGINT) as brand_id,
            CAST(NULL AS STRING) as std_brand_name,
            d.manufacturer, d.has_sku, d.variant_information,
            d.category, d.sub_category,
            CAST(NULL AS STRING) as category_id,
            CAST(NULL AS STRING) as category_name,
            CAST(NULL AS INT) as category_1_id, CAST(NULL AS STRING) as category_1_name,
            CAST(NULL AS INT) as category_2_id, CAST(NULL AS STRING) as category_2_name,
            CAST(NULL AS INT) as category_3_id, CAST(NULL AS STRING) as category_3_name,
            CAST(NULL AS INT) as category_4_id, CAST(NULL AS STRING) as category_4_name,
            CAST(NULL AS INT) as category_5_id, CAST(NULL AS STRING) as category_5_name,
            CAST(NULL AS INT) as category_6_id, CAST(NULL AS STRING) as category_6_name,
            CAST(NULL AS INT) as category_7_id, CAST(NULL AS STRING) as category_7_name,
            CAST(NULL AS INT) as category_8_id, CAST(NULL AS STRING) as category_8_name,
            CAST(NULL AS INT) as category_9_id, CAST(NULL AS STRING) as category_9_name,
            d.seller, d.seller_id, d.first_image, d.imags, d.video,
            d.specifications, d.additional_description, 
            COALESCE(ejc.extra_json_list, ARRAY()) as extra_json,
            d.etl_source, d.snapshot_time, d.region, d.dt
        FROM deduplicated d
        LEFT JOIN extra_json_collected ejc ON d.sku_id = ejc.optimized_sku_id 
            AND d.region = ejc.region AND d.dt = ejc.dt
    )
     
    SELECT * FROM final_result
    """
    
    # æ‰§è¡ŒSQLè·å–åŸºç¡€æ•°æ®
    df = execute_sql(base_query, spark.sql)
    
    # ä½¿ç”¨UDFå¤„ç†extra_jsonåˆå¹¶ï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰
    if TEST_LIMIT:
        # æµ‹è¯•æ¨¡å¼ï¼šç›´æ¥è¿”å›
        return df
    
    # ç”Ÿäº§æ¨¡å¼ï¼šå¤„ç†extra_jsonåˆå¹¶
    df.createOrReplaceTempView("base_result")
    
    # è·å–éœ€è¦åˆå¹¶extra_jsonçš„æ•°æ®
    merge_query = f"""
    SELECT 
        optimized_sku_id,
        region,
        dt,
        COLLECT_LIST(extra_json) as extra_json_list
    FROM (
        SELECT 
            FIRST_VALUE(if(sku_id IS NULL OR sku_id = '', NULL, sku_id)) IGNORE NULLS OVER (
                PARTITION BY region, dt 
                ORDER BY {priority_case_statement}, snapshot_time DESC
            ) as optimized_sku_id,
            region,
            dt,
            extra_json,
            {priority_case_statement} as priority_rank,
            snapshot_time
        FROM {UPSTREAM_TABLE}
        WHERE dt = '{calc_partition}'
          AND region IS NOT NULL AND region != ''
          AND extra_json IS NOT NULL AND extra_json != '' AND extra_json != '{empty_json}'
    ) ranked_data
    GROUP BY optimized_sku_id, region, dt
    """
    
    merge_df = execute_sql(merge_query, spark.sql)
    
    # ä½¿ç”¨UDFåˆå¹¶JSON
    merged_json_df = merge_df.withColumn(
        "merged_extra_json", 
        merge_json_udf_func("extra_json_list")
    ).select("optimized_sku_id", "region", "dt", "merged_extra_json")
    
    # åˆå¹¶ç»“æœ
    final_df = df.join(
        merged_json_df, 
        (df.sku_id == merged_json_df.optimized_sku_id) & 
        (df.region == merged_json_df.region) & 
        (df.dt == merged_json_df.dt), 
        "left"
    ).select(
        df["*"], 
        merged_json_df["merged_extra_json"]
    ).withColumn(
        "extra_json", 
        when(col("merged_extra_json").isNotNull(), col("merged_extra_json"))
        .otherwise(col("extra_json").cast("string"))
    ).drop("merged_extra_json")
    
    return final_df

def write_to_hive(spark, df):
    """å†™å…¥Hiveè¡¨ - åŠ¨æ€åˆ†åŒºè¦†ç›–å†™å…¥"""
    # æ£€æŸ¥æ•°æ®é‡
    count = df.count()
    print(f"ğŸ“Š å‡†å¤‡å†™å…¥ {TARGET_DB}.{TARGET_TABLE} çš„æ•°æ®é‡: {count}")
    
    if count == 0:
        print("âš ï¸  è­¦å‘Š: æ²¡æœ‰æ•°æ®éœ€è¦å†™å…¥ï¼Œè·³è¿‡å†™å…¥æ“ä½œ")
        return
    
    # ç¡®ä¿è¡¨å­˜åœ¨
    spark.sql(DDL)
    
    # åˆ›å»ºä¸´æ—¶è§†å›¾
    df.createOrReplaceTempView("temp_dwd_sku_daily")
    
    # ä½¿ç”¨INSERT OVERWRITEåŠ¨æ€åˆ†åŒºå†™å…¥
    overwrite_sql = f"""
    INSERT OVERWRITE TABLE {TARGET_DB}.{TARGET_TABLE}
    PARTITION (region, dt)
    SELECT 
        product_id, sku_id, product_title, url, color, size, brand, brand_id,
        std_brand_name, manufacturer, has_sku, variant_information,
        category, sub_category, category_id, category_name,
        category_1_id, category_1_name, category_2_id, category_2_name,
        category_3_id, category_3_name, category_4_id, category_4_name,
        category_5_id, category_5_name, category_6_id, category_6_name,
        category_7_id, category_7_name, category_8_id, category_8_name,
        category_9_id, category_9_name,
        seller, seller_id, first_image, imags, video,
        specifications, additional_description, extra_json,
        etl_source, snapshot_time,
        region, dt
    FROM temp_dwd_sku_daily
    """
    
    # æ‰§è¡Œè¦†ç›–å†™å…¥
    print("ğŸš€ å¼€å§‹å†™å…¥æ•°æ®åˆ°Hiveè¡¨...")
    spark.sql(overwrite_sql)
    print("âœ… æ•°æ®å†™å…¥å®Œæˆ")

# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    init_logging()
    impala = new_impala_connector()
    
    # åˆ›å»ºSparkä¼šè¯
    spark = SparkSession.builder.appName("oneflow.{{dag_name}}.{{job_name}}").enableHiveSupport().getOrCreate()
    spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    spark.sql(f'SET spark.sql.shuffle.partitions={PARTITION_NUM}')
    spark.conf.set("spark.sql.adaptive.enabled", "true")
    spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
    spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
    spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")
    
    # æ•°æ®æå–å’Œè½¬æ¢
    df = dumper_daily_sku(spark, CALC_PARTITION)
    
    # æ•°æ®åŠ è½½
    write_to_hive(spark, df)
    
    # å…³é—­Sparkä¼šè¯
    spark.stop()
    # impala.execute(f"invalidate metadata {TARGET_DB}.{TARGET_TABLE}")
    # impala.execute(f"DROP INCREMENTAL STATS {TARGET_DB}.{TARGET_TABLE} PARTITION(dt='{CALC_PARTITION}')")
    # impala.execute(f"COMPUTE INCREMENTAL STATS {TARGET_DB}.{TARGET_TABLE}")
    # åˆ·æ–°å…ƒæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
    try:
        print(f"å¼€å§‹åˆ·æ–°å…ƒæ•°æ®: {TARGET_DB}.{TARGET_TABLE}")
        impala.execute(f"invalidate metadata {TARGET_DB}.{TARGET_TABLE}")
        print("âœ… invalidate metadata æˆåŠŸ")
        
        print(f"åˆ é™¤åˆ†åŒºç»Ÿè®¡ä¿¡æ¯: dt='{CALC_PARTITION}'")
        impala.execute(f"DROP INCREMENTAL STATS {TARGET_DB}.{TARGET_TABLE} PARTITION(dt='{CALC_PARTITION}')")
        print("âœ… DROP INCREMENTAL STATS æˆåŠŸ")
        
        print("é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
        impala.execute(f"COMPUTE INCREMENTAL STATS {TARGET_DB}.{TARGET_TABLE}")
        print("âœ… COMPUTE INCREMENTAL STATS æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ å…ƒæ•°æ®åˆ·æ–°å¤±è´¥: {e}")
    
    finally:
        # ç¡®ä¿è¿æ¥å…³é—­
        try:
            impala.close()
        except:
            pass

if __name__ == "__main__":
    exit(main())
