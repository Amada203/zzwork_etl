# datahub_amazon.dim_title_translation 需求文档

## 基本信息

| 项目 | 内容 |
|------|------|
| **表名** | `datahub_amazon.dim_title_translation` |
| **清洗频率** | 日度 |
| **更新方式** | 日度全量 |
| **主键** | 无 |

## 业务逻辑

### 1. 筛选逻辑

#### 1.1 符合条件的需要进行翻译

**数据源查询**：
```sql
datahub_amazon.crawler_masterlist a
where task_type=amazon_translation
and etl_dt= 这个task_type下的max(dt)
dim_sku_ori b 

select distinct b.product_title,b.brand from b 
left semi join a on a.sku_id=b.sku_id
```

**需要翻译的字段**：
- `product_title` - 商品标题
- `brand` - 品牌（提取出来给到大模型判断）

#### 1.2 翻译过了的无需翻译

**去重依据**：使用 `Fields_hash` 作为判断是否翻译过的依据

### 2. 翻译服务配置

#### 2.1 API配置
- **服务名称**：MS-Ulanzi-Translation
- **配置方式**：从数据源配置获取API参数
- **支持模型**：从数据源配置获取模型名称
- **API URL**：从数据源配置获取base_url

#### 2.2 翻译参数

**输入给翻译的参数**：
```json
{
    "商品标题": "product_title",
    "品牌": "brand"
}
```

#### 2.3 提示词

**完整提示词格式**：
```
## 角色
精通中文、英文、德语、日文、西班牙语、葡萄牙的语言翻译

## 任务
将输入商品标题的内容翻译成纯中文。
语言可能是英文、德语、日文、西班牙语、葡萄牙语其中一种，需要根据实际情况进行甄别翻译。
如果对应商品标题原文里含有品牌对应文字则命中文字无需翻译，保留命中文字加翻译文本

## 输出要求
请直接输出翻译后的中文标题，不要包含任何其他内容。
```

**参数配置**：
- **temperature**: 0.1（控制输出稳定性）
- **max_tokens**: 1000（限制输出长度）
- **超时时间**: 30秒
- **重试机制**: 最多3次重试，指数退避

### 3. 缓存机制

**翻译需要添加缓存**：已经翻译过的无需请求大模型

### 4. 量级控制

**翻译量级控制**：
- **每日上限**：1,000,000条（100万）
- **批处理大小**：100条
- **请求间隔**：0.1秒
- **批次间隔**：0.2秒

## 表结构定义

| 字段名 | 类型 | 含义 | 主键 | 可空 |
|--------|------|------|------|------|
| **Fields** | STRING | 需要翻译的字段 | - | - |
| **Fields_in_translation** | STRING | 翻译后的字段 | - | - |
| **Fields_hash** | STRING | 哈希(需要翻译的字段) | - | - |

## 技术实现要点

### 1. 数据提取
- 从 `datahub_amazon.crawler_masterlist` 表获取符合条件的SKU列表
- 通过 `LEFT SEMI JOIN` 关联 `datahub_amazon.dim_sku_ori` 表
- 提取 `product_title` 和 `brand` 字段
- 生成MD5哈希值作为缓存键：`md5(product_title)`

### 2. 缓存机制
- **缓存表检查**：自动检测 `dim_title_translation` 表是否存在
- **缓存查询**：使用 `LEFT ANTI JOIN` 排除已翻译的记录
- **缓存键**：使用 `Fields_hash` 字段进行去重判断

### 3. 翻译处理
- **API调用**：调用豆包API进行多语言翻译
- **支持语言**：英文、德语、日文、西班牙语、葡萄牙语到中文
- **品牌保护**：保留品牌名称，只翻译其他部分
- **错误处理**：实现重试机制和指数退避策略

### 4. 性能优化
- **批处理**：每批处理100条记录
- **请求控制**：实现请求间隔和批次间隔
- **Spark优化**：设置自适应查询和分区优化
- **数据加载**：使用 `spark_write_hive` 进行高效数据写入

### 5. 监控和日志
- **处理统计**：记录提取数据量和翻译成功数量
- **错误日志**：详细记录API调用失败和重试情况
- **进度监控**：实时显示翻译进度和剩余量级

## 数据质量要求

### 1. 输入数据验证
- `product_title` 不能为空且不能为空字符串
- `brand` 不能为空且不能为空字符串
- 数据格式标准化处理

### 2. 翻译质量保证
- **多语言自动识别**：支持5种语言的自动识别
- **品牌名称保护**：保留品牌原文，只翻译其他部分
- **翻译结果验证**：确保输出为纯中文，无额外内容
- **API响应验证**：检查API响应格式和内容完整性

### 3. 输出数据规范
- **字段命名**：使用首字母大写的驼峰命名
- **数据类型**：统一使用STRING类型
- **存储格式**：PARQUET + SNAPPY压缩
- **表结构**：无分区设计，日度全量更新

## 监控指标

### 1. 处理指标
- 每日翻译数量
- 翻译成功率
- 缓存命中率

### 2. 性能指标
- API响应时间
- 处理耗时
- 资源使用率

### 3. 质量指标
- 翻译准确率
- 数据完整性
- 错误率统计

## 配置参数

### 1. 基础配置
- **目标数据库**：`datahub_amazon`
- **目标表名**：`dim_title_translation`
- **更新方式**：日度全量更新（INTO模式）
- **分区设计**：无分区

### 2. 性能配置
- **分区数**：400
- **压缩格式**：SNAPPY
- **元数据刷新**：关闭（REFRESH_STATS = False）
- **EMR环境**：启用

### 3. 翻译配置
- **批处理大小**：100条
- **请求间隔**：0.1秒
- **批次间隔**：0.2秒
- **每日上限**：1,000,000条
- **重试次数**：最多3次

## 注意事项

1. **API配置安全**：API参数从数据源配置获取，避免硬编码
2. **量级控制**：严格遵守每日100万条翻译限制
3. **数据质量**：确保输入数据的有效性和完整性
4. **成本控制**：通过缓存机制减少重复翻译，优化API调用成本
5. **错误处理**：实现完善的错误处理和重试机制，包含指数退避策略
6. **资源管理**：及时释放Spark和Impala连接资源
7. **表结构兼容**：确保DDL创建和现有表结构兼容
