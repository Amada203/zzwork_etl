# -*- coding: utf-8 -*-

import logging
import os
import pandas as pd
import numpy as np
import tempfile
from datetime import datetime, timedelta
from pigeon.connector import new_impala_connector, new_hive_connector
from pigeon.loader import new_csv_to_hive_loader
from pigeon.utils import init_logging

# ==================== åˆå§‹åŒ–æ—¥å¿— ====================
init_logging()

# ==================== é…ç½®å‚æ•° ====================
# æ•°æ®åº“é…ç½®
DB = 'lla_matrank'
SOURCE_TABLE = 'jd_weekly_dwd_sku_info'  # ç»Ÿä¸€ä½¿ç”¨åŒä¸€ä¸ªè¡¨ï¼Œé€šè¿‡åˆ†åŒºåŒºåˆ†
TARGET_TABLE = 'jd_weekly_sales_prediction'

# ä½¿ç”¨åˆ†åŒºè¡¨ï¼ˆå»ºè®®ï¼‰- ä¿ç•™å†å²é¢„æµ‹æ•°æ®
USE_PARTITION_TABLE = True  # True=åˆ†åŒºè¡¨ä¿ç•™å†å², False=è¦†ç›–æ¨¡å¼

# å“ç±»è¿‡æ»¤
CATEGORY_IDS = [
    '842', '862', '863', '12347', '12348', 
    '12350', '12352', '12417', '12597', 
    '16961', '17410', '17413', '31389', 
    '38678', '39284', '43533'
]

# è®¾ç½®å…¨å±€éšæœºç§å­ï¼Œç¡®ä¿å¯é‡ç°æ€§
np.random.seed(42)
STANDARD_DAYS = 7

# ==================== é›¶è¯„è®ºè¡°å‡ç³»æ•°é…ç½® ====================
# æ ¹æ®è¿ç»­0è¯„è®ºå‘¨æ•°åº”ç”¨è¡°å‡ç³»æ•°
# -3: è€å“æ¼é‡‡ï¼ˆæ— å†å²æ•°æ®ï¼Œä½†æ€»è¯„è®ºæ•°>=100ï¼Œå½“å‰æœ‰è¯„è®ºï¼‰
# -2: æ–°å•†å“ï¼Œå½“å‰æœ‰è¯„è®ºï¼ˆæ— å†å²æ•°æ®ï¼Œä½†å½“å‰è¯„è®ºæ•°>0ï¼Œä¸”æ€»è¯„è®ºæ•°<100ï¼‰
# -1: æ–°å•†å“ï¼Œå½“å‰æ— è¯„è®ºï¼ˆæ— å†å²æ•°æ®ï¼Œä¸”å½“å‰è¯„è®ºæ•°=0ï¼‰
# 0: è€å•†å“ï¼Œå½“å‰æœ‰è¯„è®ºï¼ˆæœ‰å†å²æ•°æ®ï¼Œä¸”å½“å‰è¯„è®ºæ•°>0ï¼‰
# 1-3: è€å•†å“ï¼Œè¿ç»­1-3å‘¨0è¯„è®º
# 4+: è€å•†å“ï¼Œè¿ç»­4å‘¨åŠä»¥ä¸Š0è¯„è®ºï¼ˆç›´æ¥è®¾ä¸º0ï¼‰
ZERO_COMMENTS_DECAY_FACTORS = {
    -3: 0.9,  # è€å“æ¼é‡‡ï¼ˆæ— å†å²æ•°æ®ï¼Œä½†æ€»è¯„è®ºæ•°>=100ï¼Œå½“å‰æœ‰è¯„è®ºï¼‰ï¼Œè½»å¾®è¡°å‡ï¼ˆé™ä½10%ï¼‰
    -2: 0.8,  # æ–°å•†å“ï¼Œå½“å‰æœ‰è¯„è®ºï¼ˆæ— å†å²æ•°æ®ï¼Œä½†å½“å‰è¯„è®ºæ•°>0ï¼Œä¸”æ€»è¯„è®ºæ•°<100ï¼‰ï¼Œè½»å¾®è°ƒæ•´
    -1: 0.3,  # æ–°å•†å“ï¼Œå½“å‰æ— è¯„è®ºï¼ˆæ— å†å²æ•°æ®ï¼Œä¸”å½“å‰è¯„è®ºæ•°=0ï¼‰ï¼Œç»™äºˆè¾ƒä½æƒé‡
    0: 1.0,   # è€å•†å“ï¼Œå½“å‰æœ‰è¯„è®ºï¼ˆæœ‰å†å²æ•°æ®ï¼Œä¸”å½“å‰è¯„è®ºæ•°>0ï¼‰ï¼Œæ­£å¸¸é¢„æµ‹ï¼Œä¸è¡°å‡
    1: 0.5,   # è€å•†å“ï¼Œè¿ç»­1å‘¨0è¯„è®ºï¼Œè½»å¾®è¡°å‡
    2: 0.2,   # è€å•†å“ï¼Œè¿ç»­2å‘¨0è¯„è®ºï¼Œä¸­ç­‰è¡°å‡
    3: 0.1,   # è€å•†å“ï¼Œè¿ç»­3å‘¨0è¯„è®ºï¼Œå¤§å¹…è¡°å‡
    4: 0.0,   # è€å•†å“ï¼Œè¿ç»­4å‘¨åŠä»¥ä¸Š0è¯„è®ºï¼Œç›´æ¥è®¾ä¸º0
}

# è€å“æ¼é‡‡åˆ¤æ–­é˜ˆå€¼ï¼šæ€»è¯„è®ºæ•° >= æ­¤å€¼è®¤ä¸ºæ˜¯è€å“æ¼é‡‡
OLD_PRODUCT_MISSING_THRESHOLD = 100

# ==================== 1. å“ç±»æ˜ å°„ ====================
CATEGORY_MAPPING = {
    '31389': 'æ™ºèƒ½å„¿ç«¥æ‰‹è¡¨',
    '17410': 'å®¶åº­å½±é™¢',
    '12347': 'æ™ºèƒ½æ‰‹ç¯',
    '862': 'æ‰‹æœºè€³æœº',
    '842': 'è“ç‰™/æ— çº¿è€³æœº',
    '12597': 'æ™ºèƒ½å¥åº·',
    '12352': 'æ™ºèƒ½é…é¥°',
    '863': 'è“ç‰™è€³æœº',
    '12348': 'æ™ºèƒ½æ‰‹è¡¨',
    '39284': 'æ™ºèƒ½é¥°å“',
    '16961': 'ç¿»è¯‘æœº/ç¿»è¯‘è®¾å¤‡',
    '17413': 'å›éŸ³å£/SoundbaréŸ³å“',
    '38678': 'æ™ºèƒ½æˆ’æŒ‡',
    '12350': 'è¿åŠ¨è·Ÿè¸ªå™¨',
    '43533': 'è¿åŠ¨æ‰‹è¡¨',
    '12417': 'å¤šåŠŸèƒ½æ‰‹è¡¨'
}

# ==================== 2. å“ç±»ç‰¹å®šç³»æ•° ====================
CATEGORY_COEFFICIENTS = {
    'æ™ºèƒ½å„¿ç«¥æ‰‹è¡¨': {'base': 0.5, 'comments_weight': 1.2, 'score_weight': 0.8},
    'æ™ºèƒ½æ‰‹è¡¨': {'base': 0.6, 'comments_weight': 1.0, 'score_weight': 0.9},
    'è“ç‰™/æ— çº¿è€³æœº': {'base': 0.7, 'comments_weight': 1.1, 'score_weight': 0.7},
    'è“ç‰™è€³æœº': {'base': 0.7, 'comments_weight': 1.1, 'score_weight': 0.7},
    'æ™ºèƒ½æ‰‹ç¯': {'base': 0.5, 'comments_weight': 1.0, 'score_weight': 0.8},
    'è¿åŠ¨æ‰‹è¡¨': {'base': 0.6, 'comments_weight': 1.0, 'score_weight': 0.9},
    'è¿åŠ¨è·Ÿè¸ªå™¨': {'base': 0.5, 'comments_weight': 1.0, 'score_weight': 0.8},
    'æ™ºèƒ½å¥åº·': {'base': 0.4, 'comments_weight': 0.9, 'score_weight': 0.7},
    'æ™ºèƒ½é…é¥°': {'base': 0.3, 'comments_weight': 0.8, 'score_weight': 0.6},
    'æ™ºèƒ½é¥°å“': {'base': 0.3, 'comments_weight': 0.8, 'score_weight': 0.6},
    'æ™ºèƒ½æˆ’æŒ‡': {'base': 0.3, 'comments_weight': 0.8, 'score_weight': 0.6},
    'å¤šåŠŸèƒ½æ‰‹è¡¨': {'base': 0.6, 'comments_weight': 1.0, 'score_weight': 0.9},
    'å®¶åº­å½±é™¢': {'base': 0.3, 'comments_weight': 0.7, 'score_weight': 0.5},
    'å›éŸ³å£/SoundbaréŸ³å“': {'base': 0.3, 'comments_weight': 0.7, 'score_weight': 0.5},
    'æ‰‹æœºè€³æœº': {'base': 0.7, 'comments_weight': 1.1, 'score_weight': 0.7},
    'ç¿»è¯‘æœº/ç¿»è¯‘è®¾å¤‡': {'base': 0.2, 'comments_weight': 0.6, 'score_weight': 0.4}
}

# ==================== 3. å·¥å…·å‡½æ•° ====================
def calculate_days_diff(current_time, previous_time):
    """
    è®¡ç®—ä¸¤ä¸ªæ—¶é—´ç‚¹çš„å¤©æ•°å·®ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰
    """
    if pd.isna(current_time) or pd.isna(previous_time):
        return STANDARD_DAYS
    
    if isinstance(current_time, str):
        try:
            current_time = datetime.strptime(current_time, '%Y-%m-%d %H:%M:%S')
        except:
            return STANDARD_DAYS
    
    if isinstance(previous_time, str):
        try:
            previous_time = datetime.strptime(previous_time, '%Y-%m-%d %H:%M:%S')
        except:
            return STANDARD_DAYS
    
    time_diff = current_time - previous_time
    # è½¬æ¢ä¸ºå¤©æ•°ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰
    # åŸç†ï¼š1å¤© = 86400ç§’ï¼ˆ24å°æ—¶ Ã— 60åˆ†é’Ÿ Ã— 60ç§’ï¼‰
    # ç§’æ•°å·® / 86400 = å¤©æ•°å·®
    # ä¾‹å¦‚ï¼š43200ç§’ = 12å°æ—¶ = 0.5å¤©
    # ä¿ç•™2ä½å°æ•°ï¼š4.3015625å¤© -> 4.30å¤©ï¼Œè¯¯å·®<0.05%
    days_diff = round(time_diff.total_seconds() / 86400.0, 2)
    
    if days_diff <= 0:
        return STANDARD_DAYS
    
    return days_diff

def normalize_to_standard_days(sales_value, data_days, standard_days=STANDARD_DAYS):
    """å°†é”€é‡å½’ä¸€åŒ–åˆ°æ ‡å‡†å¤©æ•°"""
    if data_days is None or data_days <= 0:
        return sales_value
    return sales_value * standard_days / data_days

def apply_category_adjustment(predictions, category_name, comments_number):
    """åº”ç”¨å“ç±»ç‰¹å®šçš„è°ƒæ•´ç³»æ•°"""
    if category_name in CATEGORY_COEFFICIENTS:
        coeff = CATEGORY_COEFFICIENTS[category_name]
        adjustment = (coeff['base'] + 
                     np.log1p(comments_number) / 100 * coeff['comments_weight'])
        return predictions * adjustment
    return predictions

def calculate_previous_week_dt(current_next_week_dt):
    # å°è¯•è§£ææ—¥æœŸæ ¼å¼
    date_format = None
    try:
        # å°è¯• 'YYYY-MM-DD' æ ¼å¼
        dt = datetime.strptime(current_next_week_dt, '%Y-%m-%d')
        date_format = '%Y-%m-%d'
    except:
        try:
            # å°è¯• 'YYYYMMDD' æ ¼å¼
            dt = datetime.strptime(current_next_week_dt, '%Y%m%d')
            date_format = '%Y%m%d'
        except:
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨è¯†åˆ«æ ¼å¼
            if '-' in current_next_week_dt:
                dt = datetime.strptime(current_next_week_dt, '%Y-%m-%d')
                date_format = '%Y-%m-%d'
            else:
                dt = datetime.strptime(current_next_week_dt, '%Y%m%d')
                date_format = '%Y%m%d'
    
    # å‡å»7å¤©
    previous_dt = dt - timedelta(days=7)
    
    # è¿”å›ç›¸åŒæ ¼å¼
    return previous_dt.strftime(date_format)

def calculate_previous_weeks_dt(current_next_week_dt, weeks=3):
    """
    è®¡ç®—å‰Nå‘¨çš„next_week_dt
    
    Args:
        current_next_week_dt: å½“å‰next_week_dt
        weeks: å¾€å‰æ¨çš„å‘¨æ•°ï¼ˆé»˜è®¤3å‘¨ï¼‰
    
    Returns:
        list: [å‰1å‘¨, å‰2å‘¨, å‰3å‘¨, ...] çš„next_week_dtåˆ—è¡¨
    """
    previous_weeks = []
    date_format = None
    
    # è§£ææ—¥æœŸæ ¼å¼
    try:
        dt = datetime.strptime(current_next_week_dt, '%Y-%m-%d')
        date_format = '%Y-%m-%d'
    except:
        try:
            dt = datetime.strptime(current_next_week_dt, '%Y%m%d')
            date_format = '%Y%m%d'
        except:
            if '-' in current_next_week_dt:
                dt = datetime.strptime(current_next_week_dt, '%Y-%m-%d')
                date_format = '%Y-%m-%d'
            else:
                dt = datetime.strptime(current_next_week_dt, '%Y%m%d')
                date_format = '%Y%m%d'
    
    # è®¡ç®—å‰Nå‘¨
    for i in range(1, weeks + 1):
        prev_dt = dt - timedelta(days=7 * i)
        previous_weeks.append(prev_dt.strftime(date_format))
    
    return previous_weeks

def calculate_consecutive_zero_weeks(comments_number, prev1_comments, prev2_comments, prev3_comments, total_comments_number=None):
    """
    è®¡ç®—è¿ç»­0è¯„è®ºçš„å‘¨æ•°ï¼ŒåŒæ—¶è¯†åˆ«æ–°å•†å“å’Œè€å“æ¼é‡‡
    
    Args:
        comments_number: å½“å‰å‘¨è¯„è®ºæ•°
        prev1_comments: å‰1å‘¨è¯„è®ºæ•°ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
        prev2_comments: å‰2å‘¨è¯„è®ºæ•°ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
        prev3_comments: å‰3å‘¨è¯„è®ºæ•°ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
        total_comments_number: æ€»è¯„è®ºæ•°ï¼ˆç”¨äºåˆ¤æ–­è€å“æ¼é‡‡ï¼Œå¯é€‰ï¼‰
    
    Returns:
        int: 
            -3: è€å“æ¼é‡‡ï¼ˆæ— å†å²æ•°æ®ï¼Œä½†æ€»è¯„è®ºæ•°>=100ï¼Œå½“å‰æœ‰è¯„è®ºï¼‰
            -2: æ–°å•†å“ï¼Œå½“å‰æœ‰è¯„è®ºï¼ˆæ— å†å²æ•°æ®ï¼Œä½†å½“å‰è¯„è®ºæ•°>0ï¼Œä¸”æ€»è¯„è®ºæ•°<100ï¼‰
            -1: æ–°å•†å“ï¼Œå½“å‰æ— è¯„è®ºï¼ˆæ— å†å²æ•°æ®ï¼Œä¸”å½“å‰è¯„è®ºæ•°=0ï¼‰
            0: è€å•†å“ï¼Œå½“å‰æœ‰è¯„è®ºï¼ˆæœ‰å†å²æ•°æ®ï¼Œä¸”å½“å‰è¯„è®ºæ•°>0ï¼‰
            1-3: è€å•†å“ï¼Œè¿ç»­1-3å‘¨0è¯„è®º
            4: è€å•†å“ï¼Œè¿ç»­4å‘¨åŠä»¥ä¸Š0è¯„è®º
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰å†å²æ•°æ®
    has_history = not (prev1_comments is None or pd.isna(prev1_comments))
    
    # å¦‚æœå½“å‰å‘¨æœ‰è¯„è®º
    if comments_number > 0:
        if not has_history:
            # æ— å†å²æ•°æ®ï¼Œä½†æœ‰è¯„è®º
            # åˆ¤æ–­æ˜¯å¦ä¸ºè€å“æ¼é‡‡ï¼šæ€»è¯„è®ºæ•° >= OLD_PRODUCT_MISSING_THRESHOLD è®¤ä¸ºæ˜¯è€å“æ¼é‡‡
            if total_comments_number is not None and total_comments_number >= OLD_PRODUCT_MISSING_THRESHOLD:
                return -3  # è€å“æ¼é‡‡ï¼Œè½»å¾®è¡°å‡ï¼ˆ0.9ï¼‰
            else:
                return -2  # æ–°å•†å“ï¼Œå½“å‰æœ‰è¯„è®º
        else:
            return 0   # è€å•†å“ï¼Œå½“å‰æœ‰è¯„è®ºï¼ˆä¸è¡°å‡ï¼‰
    
    # å¦‚æœå½“å‰å‘¨æ— è¯„è®º
    if not has_history:
        return -1  # æ–°å•†å“ï¼Œå½“å‰æ— è¯„è®º
    
    # è€å•†å“ï¼Œå½“å‰æ— è¯„è®ºï¼Œæ£€æŸ¥è¿ç»­0è¯„è®ºå‘¨æ•°
    # æ³¨æ„ï¼šæ­¤æ—¶ has_history = Trueï¼Œæ‰€ä»¥ prev1_comments ä¸æ˜¯ None ä¹Ÿä¸æ˜¯ NaN
    # ä½† prev1_comments å¯èƒ½æ˜¯ 0ï¼ˆæ•°å€¼0ï¼‰
    if prev1_comments is not None and not pd.isna(prev1_comments) and prev1_comments > 0:
        return 1  # è¿ç»­1å‘¨0è¯„è®º
    
    # prev1_comments = 0 æˆ–ä¸å­˜åœ¨ï¼Œç»§ç»­æ£€æŸ¥ prev2_comments
    if prev2_comments is None or pd.isna(prev2_comments):
        return 1  # åªæœ‰1å‘¨å†å²æ•°æ®ï¼Œä¸”ä¸º0
    
    if prev2_comments is not None and not pd.isna(prev2_comments) and prev2_comments > 0:
        return 2  # è¿ç»­2å‘¨0è¯„è®º
    
    # prev2_comments = 0ï¼Œç»§ç»­æ£€æŸ¥ prev3_comments
    if prev3_comments is None or pd.isna(prev3_comments):
        return 2  # åªæœ‰2å‘¨å†å²æ•°æ®ï¼Œä¸”éƒ½ä¸º0
    
    if prev3_comments is not None and not pd.isna(prev3_comments) and prev3_comments > 0:
        return 3  # è¿ç»­3å‘¨0è¯„è®º
    
    # å‰3å‘¨éƒ½æ˜¯0ï¼Œè¿”å›4ï¼ˆè¡¨ç¤º4å‘¨åŠä»¥ä¸Šï¼‰
    return 4

def apply_zero_comments_decay(predicted_sales, consecutive_zero_weeks, comments_number):
    """
    åº”ç”¨è¿ç»­0è¯„è®ºè¡°å‡ç³»æ•°ï¼ˆåŒ…å«æ–°å•†å“è°ƒæ•´ï¼‰
    
    Args:
        predicted_sales: åŸå§‹é¢„æµ‹é”€é‡
        consecutive_zero_weeks: 
            -3: è€å“æ¼é‡‡
            -2: æ–°å•†å“ï¼Œå½“å‰æœ‰è¯„è®º
            -1: æ–°å•†å“ï¼Œå½“å‰æ— è¯„è®º
            0: è€å•†å“ï¼Œå½“å‰æœ‰è¯„è®º
            1-4: è€å•†å“ï¼Œè¿ç»­0è¯„è®ºå‘¨æ•°
        comments_number: å½“å‰æ€»è¯„è®ºæ•°ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼Œä¸å†ç”¨äºåˆ¤æ–­ï¼‰
    
    Returns:
        float: è°ƒæ•´åçš„é¢„æµ‹é”€é‡
    """
    # è·å–è¡°å‡ç³»æ•°
    decay_factor = ZERO_COMMENTS_DECAY_FACTORS.get(
        consecutive_zero_weeks, 
        0.0  # é»˜è®¤ï¼š4å‘¨ä»¥ä¸Šç›´æ¥ä¸º0
    )
    
    # åº”ç”¨è¡°å‡
    return predicted_sales * decay_factor

# ==================== 4. æ ¸å¿ƒé¢„æµ‹å‡½æ•° ====================
def fit_weekly_sales_from_reviews(comments_df, data_days_col='data_days'):
    """
    ä»è¯„è®ºæ•°æ®æ‹Ÿåˆå‘¨åº¦é”€é‡
    
    Args:
        comments_df: DataFrameï¼ŒåŒ…å«åˆ—ï¼šcategory_id, comments_number, comments_score, data_days
        
    Returns:
        result_df: DataFrameï¼ŒåŒ…å«é¢„æµ‹çš„å‘¨åº¦é”€é‡
    """
    result_list = []
    
    # æŒ‰å“ç±»å¤„ç†
    for category_id, category_name in CATEGORY_MAPPING.items():
        category_data = comments_df[comments_df['category_id'] == category_id].copy()
        
        if len(category_data) == 0:
            continue
        
        # å®‰å…¨å¤„ç†è¯„è®ºæ•°ï¼šä¸¤çº§é€»è¾‘ï¼ˆæ•°æ®é‡‡é›†é¢‘ç‡æ¯å‘¨ä¸€æ¬¡ï¼‰
        # æ•°æ®ç°å®ï¼šé›¶å€¼å æ¯”>90%ï¼Œè¿™æ˜¯å¸¸æ€è€Œéå¼‚å¸¸ï¼ˆæ•°æ®é‡‡é›†é¢‘ç‡ä½ï¼‰
        # é€»è¾‘ä¼˜å…ˆçº§ï¼š
        #   1. diff > 0: ä½¿ç”¨diffï¼ˆè¯„è®ºå¢é•¿ï¼Œæ­£å¸¸é”€å”®ï¼Œç¬¦åˆ7å¤©é”€é‡ç›®æ ‡ï¼‰
        #   2. diff â‰¤ 0: å›é€€åˆ°totalï¼ˆæ•°æ®é‡‡é›†é¢‘ç‡ä½ï¼Œé›¶å€¼æ˜¯å¸¸æ€ï¼‰
        # è¯´æ˜ï¼šç”±äºæ•°æ®é‡‡é›†é¢‘ç‡ä¸ºæ¯å‘¨ä¸€æ¬¡ï¼Œdiff=0æ˜¯æ­£å¸¸ä¸šåŠ¡ç°è±¡ï¼Œåº”è¯¥ç”¨totalè€Œéå°æ¯”ä¾‹
        if 'comments_number_diff' in category_data.columns:
            comments_diff = category_data['comments_number_diff'].values
            comments_total = category_data['comments_number'].values
            
            # diff > 0: ä½¿ç”¨å¢é‡ï¼ˆç¬¦åˆ7å¤©é”€é‡ç›®æ ‡ï¼‰
            # diff â‰¤ 0: å›é€€åˆ°æ€»é‡ï¼ˆæ•°æ®é‡‡é›†é¢‘ç‡ä½ï¼Œé›¶å€¼æ˜¯å¸¸æ€ï¼‰
            safe_comments = np.where(
                comments_diff > 0,
                comments_diff,
                comments_total
            )
        else:
            # å¦‚æœæ²¡æœ‰diffåˆ—ï¼Œç›´æ¥ç”¨total
            safe_comments = category_data['comments_number'].values
        
        # åŸºç¡€é”€é‡é¢„æµ‹ï¼ˆä½¿ç”¨safe_commentsï¼Œå³diffä¼˜å…ˆï¼‰
        base_sales = (
            np.log1p(safe_comments) * 10 +
            category_data['comments_score'].values * 5
        )
        
        # åº”ç”¨å“ç±»è°ƒæ•´ï¼ˆä½¿ç”¨æ€»è¯„è®ºæ•°ï¼Œå“ç±»ç³»æ•°åŸºäºå†å²æ€»è¯„è®ºæ•°ï¼‰
        # æ³¨æ„ï¼šå“ç±»è°ƒæ•´åº”è¯¥åŸºäºæ€»è¯„è®ºæ•°è€Œéå¢é‡ï¼Œå› ä¸ºå“ç±»ç³»æ•°åæ˜ çš„æ˜¯å“ç±»æ•´ä½“ç‰¹æ€§
        adjusted_sales = apply_category_adjustment(
            base_sales, 
            category_name, 
            category_data['comments_number'].values
        )
        
        # å½’ä¸€åŒ–åˆ°7å¤©
        if data_days_col in category_data.columns:
            data_days = category_data[data_days_col].values
            normalized_sales = np.array([
                normalize_to_standard_days(adj_sales, days) 
                for adj_sales, days in zip(adjusted_sales, data_days)
            ])
        else:
            normalized_sales = adjusted_sales
        
        # ========== æ–°å¢ï¼šé›¶è¯„è®ºè¡°å‡é€»è¾‘ ==========
        # è®¡ç®—è¿ç»­0è¯„è®ºå‘¨æ•°
        if all(col in category_data.columns for col in ['comments_number_prev1', 'comments_number_prev2', 'comments_number_prev3']):
            consecutive_zero_weeks = [
                calculate_consecutive_zero_weeks(
                    row['comments_number'],
                    row.get('comments_number_prev1'),
                    row.get('comments_number_prev2'),
                    row.get('comments_number_prev3'),
                    row.get('comments_number')  # ä¼ å…¥æ€»è¯„è®ºæ•°ï¼Œç”¨äºåˆ¤æ–­è€å“æ¼é‡‡
                )
                for _, row in category_data.iterrows()
            ]
        elif 'comments_number_prev1' in category_data.columns:
            # å¦‚æœåªæœ‰å‰1å‘¨æ•°æ®
            consecutive_zero_weeks = [
                calculate_consecutive_zero_weeks(
                    row['comments_number'],
                    row.get('comments_number_prev1'),
                    None,
                    None,
                    row.get('comments_number')  # ä¼ å…¥æ€»è¯„è®ºæ•°ï¼Œç”¨äºåˆ¤æ–­è€å“æ¼é‡‡
                )
                for _, row in category_data.iterrows()
            ]
        else:
            # å¦‚æœæ²¡æœ‰å†å²æ•°æ®åˆ—ï¼Œé»˜è®¤è®¤ä¸ºæ— å†å²æ•°æ®
            # ä½†éœ€è¦åˆ¤æ–­æ˜¯å¦ä¸ºè€å“æ¼é‡‡ï¼ˆæ€»è¯„è®ºæ•° >= OLD_PRODUCT_MISSING_THRESHOLDï¼‰
            consecutive_zero_weeks = [
                calculate_consecutive_zero_weeks(
                    row['comments_number'],
                    None,
                    None,
                    None,
                    row.get('comments_number')  # ä¼ å…¥æ€»è¯„è®ºæ•°ï¼Œç”¨äºåˆ¤æ–­è€å“æ¼é‡‡
                )
                for _, row in category_data.iterrows()
            ]
        
        # åº”ç”¨é›¶è¯„è®ºè¡°å‡
        normalized_sales = np.array([
            apply_zero_comments_decay(
                normalized_sales[idx],
                consecutive_zero_weeks[idx],
                row['comments_number']
            )
            for idx, (_, row) in enumerate(category_data.iterrows())
        ])
        # ========== é›¶è¯„è®ºè¡°å‡é€»è¾‘ç»“æŸ ==========
        
        # ä¿å­˜ç»“æœ
        for idx, (_, row) in enumerate(category_data.iterrows()):
            result_row = {
                'category_id': category_id,
                'category_name': category_name,
                'comments_number': row['comments_number'],
                'comments_score': row['comments_score'],
                'predicted_weekly_sales': normalized_sales[idx],
                'data_days': row.get(data_days_col, STANDARD_DAYS)
            }
            # æ·»åŠ  sku_idï¼ˆå¦‚æœå­˜åœ¨ï¼‰ç”¨äºåç»­åŒ¹é…
            if 'sku_id' in category_data.columns:
                result_row['sku_id'] = row['sku_id']
            result_list.append(result_row)
    
    return pd.DataFrame(result_list)

# ==================== 5. æ•°æ®æŸ¥è¯¢å’Œå¤„ç† ====================
def get_source_query(current_next_week_dt, previous_next_week_dt, previous_weeks_dt_list=None):
    """
    æ„å»ºæ•°æ®æºæŸ¥è¯¢SQLï¼ˆåŒ…å«å†å²æ•°æ®ï¼‰
    
    Args:
        current_next_week_dt: å½“å‰ next_week_dt åˆ†åŒºå€¼
        previous_next_week_dt: ä¸Šä¸€å‘¨ next_week_dt åˆ†åŒºå€¼ï¼ˆç”¨äºè®¡ç®—diffï¼‰
        previous_weeks_dt_list: å‰Nå‘¨çš„next_week_dtåˆ—è¡¨ [å‰1å‘¨, å‰2å‘¨, å‰3å‘¨]ï¼ˆç”¨äºé›¶è¯„è®ºè¡°å‡ï¼‰
    """
    # æ„å»ºå†å²æ•°æ®LEFT JOINå­å¥
    join_clauses = []
    select_clauses = []
    
    if previous_weeks_dt_list:
        if len(previous_weeks_dt_list) > 0:
            prev1_dt = previous_weeks_dt_list[0]
            join_clauses.append(f"""
        LEFT JOIN {DB}.{SOURCE_TABLE} b1 
            ON a.product_id = b1.product_id
            AND b1.next_week_dt = '{prev1_dt}'
            """)
            select_clauses.append("IFNULL(b1.comments_number, NULL) AS comments_number_prev1")
        
        if len(previous_weeks_dt_list) > 1:
            prev2_dt = previous_weeks_dt_list[1]
            join_clauses.append(f"""
        LEFT JOIN {DB}.{SOURCE_TABLE} b2 
            ON a.product_id = b2.product_id
            AND b2.next_week_dt = '{prev2_dt}'
            """)
            select_clauses.append("IFNULL(b2.comments_number, NULL) AS comments_number_prev2")
        
        if len(previous_weeks_dt_list) > 2:
            prev3_dt = previous_weeks_dt_list[2]
            join_clauses.append(f"""
        LEFT JOIN {DB}.{SOURCE_TABLE} b3 
            ON a.product_id = b3.product_id
            AND b3.next_week_dt = '{prev3_dt}'
            """)
            select_clauses.append("IFNULL(b3.comments_number, NULL) AS comments_number_prev3")
    
    # æ„å»ºSELECTå­å¥ï¼ˆå†å²æ•°æ®åˆ—ï¼‰
    history_select = ', '.join(select_clauses) if select_clauses else ''
    if history_select:
        history_select = ', ' + history_select
    
    sql = f"""
    SELECT 
        a.sku_id,
        a.product_id,
        a.category_id,
        a.category_name,
        a.comments_number,
        a.comments_score,
        a.original_price,
        a.purchase_price,
        a.dt,
        a.next_week_dt,
        a.comments_number - IFNULL(b.comments_number, 0) AS comments_number_diff,
        ROUND(CAST((UNIX_TIMESTAMP(a.snapshot_time, 'yyyy-MM-dd HH:mm:ss') - 
                    UNIX_TIMESTAMP(IFNULL(b.snapshot_time, a.snapshot_time), 'yyyy-MM-dd HH:mm:ss')) AS DOUBLE) / 86400.0, 2) AS days_diff{history_select}
    FROM {DB}.{SOURCE_TABLE} a
    LEFT JOIN {DB}.{SOURCE_TABLE} b 
        ON a.product_id = b.product_id
        AND b.next_week_dt = '{previous_next_week_dt}'
    {''.join(join_clauses)}
    WHERE a.next_week_dt = '{current_next_week_dt}'
        AND a.category_id IN ({','.join([f"'{cat}'" for cat in CATEGORY_IDS])})
    """
    return sql

def get_current_next_week_dt(impala):
    """
    ä»æºè¡¨ä¸­è·å–æœ€æ–°çš„ next_week_dt åˆ†åŒºå€¼
    
    Returns:
        current_next_week_dt: å½“å‰æœ€æ–°çš„ next_week_dt åˆ†åŒºå€¼ï¼Œå­—ç¬¦ä¸²æ ¼å¼
    """
    sql = f"""
    SELECT MAX(next_week_dt) as max_next_week_dt
    FROM {DB}.{SOURCE_TABLE}
    """
    rows = impala.fetchall(sql)
    if not rows or not rows[0][0]:
        raise ValueError(f"æ— æ³•ä» {DB}.{SOURCE_TABLE} è·å–æœ€æ–°çš„ next_week_dt åˆ†åŒºå€¼")
    
    current_next_week_dt = rows[0][0]
    logging.info(f"âœ“ è·å–åˆ°å½“å‰ next_week_dt: {current_next_week_dt}")
    return current_next_week_dt

def create_target_table(impala, use_partition):
    """åˆ›å»ºç›®æ ‡è¡¨"""
    if use_partition:
        # åˆ†åŒºè¡¨æ¨¡å¼ï¼šä½¿ç”¨ next_week_dt ä½œä¸ºåˆ†åŒºå­—æ®µ
        ddl = f"""
        CREATE TABLE {DB}.{TARGET_TABLE} (
            sku_id STRING COMMENT 'SKU ID',
            category_id STRING COMMENT 'å“ç±»ID',
            category_name STRING COMMENT 'å“ç±»åç§°',
            comments_number BIGINT COMMENT 'æ€»è¯„è®ºæ•°',
            comments_score DOUBLE COMMENT 'ç»¼åˆè¯„åˆ†',
            predicted_weekly_sales BIGINT COMMENT 'é¢„æµ‹çš„å‘¨åº¦é”€é‡ï¼ˆå½’ä¸€åŒ–åˆ°7å¤©ï¼‰',
            comments_number_diff BIGINT COMMENT 'è¯„è®ºæ•°å¢é‡ï¼ˆå·®å€¼ï¼‰',
            days_diff DOUBLE COMMENT 'æ•°æ®æ—¶é—´å·®ï¼ˆå¤©æ•°ï¼‰',
            dt STRING COMMENT 'æ—¥æœŸ',
            prediction_dt STRING COMMENT 'é¢„æµ‹æ—¥æœŸï¼ˆæ›´æ–°æ—¶é—´ï¼‰'
        )
        PARTITIONED BY (next_week_dt STRING COMMENT 'ä¸‹å‘¨ä¸€åˆ†åŒºï¼ˆä¸šåŠ¡åˆ†åŒºï¼‰')
        STORED AS PARQUET
        """
        
        # åˆ†åŒºè¡¨æ¨¡å¼ï¼šåªåˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼Œä¿ç•™å†å²æ•°æ®
        logging.info(f"æ£€æŸ¥åˆ†åŒºè¡¨æ˜¯å¦å­˜åœ¨: {DB}.{TARGET_TABLE}")
        try:
            impala.execute(f"DESCRIBE {DB}.{TARGET_TABLE}")
            logging.info(f"âœ“ åˆ†åŒºè¡¨å·²å­˜åœ¨ï¼Œä¿ç•™å†å²æ•°æ®")
        except:
            logging.info(f"åˆ†åŒºè¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è¡¨")
            impala.execute(ddl)
    else:
        # è¦†ç›–æ¨¡å¼ï¼šæ™®é€šè¡¨
        ddl = f"""
        CREATE TABLE {DB}.{TARGET_TABLE} (
            sku_id STRING COMMENT 'SKU ID',
            category_id STRING COMMENT 'å“ç±»ID',
            category_name STRING COMMENT 'å“ç±»åç§°',
            comments_number BIGINT COMMENT 'æ€»è¯„è®ºæ•°',
            comments_score DOUBLE COMMENT 'ç»¼åˆè¯„åˆ†',
            predicted_weekly_sales BIGINT COMMENT 'é¢„æµ‹çš„å‘¨åº¦é”€é‡ï¼ˆå½’ä¸€åŒ–åˆ°7å¤©ï¼‰',
            comments_number_diff BIGINT COMMENT 'è¯„è®ºæ•°å¢é‡ï¼ˆå·®å€¼ï¼‰',
            days_diff DOUBLE COMMENT 'æ•°æ®æ—¶é—´å·®ï¼ˆå¤©æ•°ï¼‰',
            dt STRING COMMENT 'æ—¥æœŸ',
            prediction_dt STRING COMMENT 'é¢„æµ‹æ—¥æœŸï¼ˆæ›´æ–°æ—¶é—´ï¼‰',
            next_week_dt STRING COMMENT 'ä¸‹å‘¨ä¸€'
        )
        STORED AS PARQUET
        """
        
        # è¦†ç›–æ¨¡å¼ï¼šåˆ é™¤å¹¶é‡å»ºè¡¨
        logging.info(f"è¦†ç›–æ¨¡å¼ï¼šåˆ é™¤å¹¶é‡å»ºè¡¨: {DB}.{TARGET_TABLE}")
        try:
            impala.execute(f"DROP TABLE IF EXISTS {DB}.{TARGET_TABLE}")
            logging.info(f"âœ“ æ—§è¡¨å·²åˆ é™¤")
        except Exception as e:
            logging.warning(f"åˆ é™¤æ—§è¡¨æ—¶å‡ºé”™: {e}")
        
        impala.execute(ddl)
        logging.info(f"âœ“ æ™®é€šè¡¨å·²åˆ›å»º")

# ==================== 6. ä¸»å¤„ç†æµç¨‹ ====================
def main():
    """ä¸»å¤„ç†æµç¨‹"""
    impala = None
    hive = None
    temp_table = None
    csv_path = None
    merged_temp = None  # ç”¨äºå­˜å‚¨æ–‡ä»¶åˆå¹¶çš„ä¸´æ—¶è¡¨å
    
    try:
        # 1. åˆå§‹åŒ–è¿æ¥
        logging.info("åˆå§‹åŒ–Impalaè¿æ¥...")
        impala = new_impala_connector(emr=True)
        logging.info("âœ“ è¿æ¥æˆåŠŸ")
        
        # 2. åˆ›å»ºç›®æ ‡è¡¨
        create_target_table(impala, USE_PARTITION_TABLE)
        
        # è·å–å½“å‰é¢„æµ‹æ—¥æœŸ
        current_date = datetime.now().strftime('%Y%m%d')
        
        # 3. è·å–å½“å‰å’Œä¸Šä¸€å‘¨çš„ next_week_dt
        logging.info("è·å–å½“å‰ next_week_dt åˆ†åŒºå€¼...")
        current_next_week_dt = get_current_next_week_dt(impala)
        previous_next_week_dt = calculate_previous_week_dt(current_next_week_dt)
        
        # è®¡ç®—å‰3å‘¨çš„next_week_dtï¼ˆç”¨äºé›¶è¯„è®ºè¡°å‡é€»è¾‘ï¼‰
        previous_weeks_dt_list = calculate_previous_weeks_dt(current_next_week_dt, weeks=3)
        logging.info(f"âœ“ å½“å‰ next_week_dt: {current_next_week_dt}")
        logging.info(f"âœ“ ä¸Šä¸€å‘¨ next_week_dt: {previous_next_week_dt}")
        logging.info(f"âœ“ å†å²å‘¨æ•°: {previous_weeks_dt_list}")
        
        # 4. è¯»å–æ•°æ®ï¼ˆåŒ…å«å†å²æ•°æ®ï¼‰
        logging.info("ä»Impalaè¯»å–æ•°æ®ï¼ˆåŒ…å«å†å²3å‘¨æ•°æ®ï¼‰...")
        sql = get_source_query(current_next_week_dt, previous_next_week_dt, previous_weeks_dt_list)
        rows = impala.fetchall(sql)
        logging.info(f"âœ“ æˆåŠŸè¯»å– {len(rows)} æ¡è®°å½•")
        
        if len(rows) == 0:
            logging.warning("æ²¡æœ‰æ•°æ®éœ€è¦å¤„ç†")
            return
        
        # 5. è½¬æ¢ä¸ºDataFrame
        # æ³¨æ„ï¼šSQLæŸ¥è¯¢åŒ…å«product_idï¼Œä½†åªåœ¨joinæ—¶ä½¿ç”¨ï¼Œä¸è¾“å‡ºåˆ°æœ€ç»ˆè¡¨
        columns = [
            'sku_id', 'product_id', 'category_id', 'category_name', 
            'comments_number', 'comments_score', 'original_price', 'purchase_price',
            'dt', 'next_week_dt', 'comments_number_diff', 'days_diff'
        ]
        
        # æ·»åŠ å†å²æ•°æ®åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if previous_weeks_dt_list:
            if len(previous_weeks_dt_list) > 0:
                columns.append('comments_number_prev1')
            if len(previous_weeks_dt_list) > 1:
                columns.append('comments_number_prev2')
            if len(previous_weeks_dt_list) > 2:
                columns.append('comments_number_prev3')
        
        df = pd.DataFrame(rows, columns=columns)
        # åˆ é™¤product_idåˆ—ï¼ˆä¸sku_idé‡å¤ï¼‰
        if 'product_id' in df.columns:
            df = df.drop(columns=['product_id'])
        logging.info(f"âœ“ DataFrameåˆ›å»ºæˆåŠŸï¼ŒShape: {df.shape}")
        
        # 6. æ‰§è¡Œé¢„æµ‹
        logging.info("æ‰§è¡Œé”€é‡é¢„æµ‹...")
        
        # æ•°æ®éªŒè¯ï¼šæ£€æŸ¥è¯„è®ºæ•°å¢é‡
        neg_count = (df['comments_number_diff'] < 0).sum()
        zero_count = (df['comments_number_diff'] == 0).sum()
        pos_count = (df['comments_number_diff'] > 0).sum()
        total_count = len(df)
        
        if neg_count > 0 or zero_count > 0:
            logging.warning(f"âš ï¸ è¯„è®ºæ•°å·®å€¼ç»Ÿè®¡ï¼š")
            logging.warning(f"   æ­£æ•°: {pos_count}æ¡ ({pos_count/total_count*100:.2f}%)")
            logging.warning(f"   é›¶å€¼: {zero_count}æ¡ ({zero_count/total_count*100:.2f}%)")
            logging.warning(f"   è´Ÿæ•°: {neg_count}æ¡ ({neg_count/total_count*100:.2f}%)")
            
            if zero_count / total_count > 0.5:
                logging.warning(f"   âš ï¸ é›¶å€¼å æ¯”è¶…è¿‡50%ï¼Œè¯´æ˜å¤§éƒ¨åˆ†å•†å“æœ¬å‘¨æ— æ–°å¢è¯„è®ºï¼ˆæ•°æ®é‡‡é›†é¢‘ç‡æ¯å‘¨ä¸€æ¬¡ï¼‰")
            
            logging.info(f"   é¢„æµ‹ç­–ç•¥ï¼ˆä¸¤çº§é€»è¾‘ï¼Œæ•°æ®é‡‡é›†é¢‘ç‡æ¯å‘¨ä¸€æ¬¡ï¼‰ï¼š")
            logging.info(f"     1. diff>0: ä½¿ç”¨å·®å€¼ï¼ˆè¯„è®ºå¢é•¿ï¼Œæ­£å¸¸é”€å”®ï¼Œç¬¦åˆ7å¤©é”€é‡ç›®æ ‡ï¼‰")
            logging.info(f"     2. diffâ‰¤0: å›é€€åˆ°æ€»è¯„è®ºæ•°ï¼ˆæ•°æ®é‡‡é›†é¢‘ç‡ä½ï¼Œé›¶å€¼æ˜¯å¸¸æ€ï¼‰")
        
        result_df = fit_weekly_sales_from_reviews(df, data_days_col='days_diff')
        logging.info(f"âœ“ é¢„æµ‹å®Œæˆï¼Œç”Ÿæˆ {len(result_df)} æ¡ç»“æœ")
        
        # 7. å‡†å¤‡è¾“å‡ºæ•°æ®
        logging.info("å‡†å¤‡å†™å…¥æ•°æ®...")
        
        # å…³é”®ï¼šç¡®ä¿ result_df å’Œ df çš„è¡Œä¸€ä¸€å¯¹åº”
        # ç”±äº fit_weekly_sales_from_reviews æŒ‰å“ç±»å¤„ç†ï¼Œresult_df çš„é¡ºåºä¸ df ä¸åŒ
        # æ–¹æ³•ï¼šä½¿ç”¨ sku_id ä» df ä¸­æå–å¯¹åº”è¡Œçš„å…¶ä»–å­—æ®µ
        
        # ä» result_df ä¸­æå–é¢„æµ‹ç›¸å…³çš„åˆ—ï¼ˆä¸åŒ…æ‹¬ comments_numberï¼Œå› ä¸ºè¦ä½¿ç”¨æ€»è¯„è®ºæ•°ï¼‰
        prediction_cols = ['sku_id', 'category_id', 'category_name', 
                          'comments_score', 'predicted_weekly_sales']
        output_df = result_df[prediction_cols].copy()
        
        # å°† predicted_weekly_sales è½¬æ¢ä¸ºæ•´æ•°ï¼ˆBIGINTï¼‰
        # å¤„ç† NaN å’Œ inf å€¼ï¼Œæ›¿æ¢ä¸º 0
        before_fill = output_df['predicted_weekly_sales'].isna().sum() + (output_df['predicted_weekly_sales'] == np.inf).sum() + (output_df['predicted_weekly_sales'] == -np.inf).sum()
        
        output_df['predicted_weekly_sales'] = output_df['predicted_weekly_sales'].replace([np.inf, -np.inf], np.nan).fillna(0).round().astype(int)
        
        if before_fill > 0:
            logging.warning(f"âš ï¸ é¢„æµ‹é”€é‡å¼‚å¸¸å€¼ç»Ÿè®¡ï¼šNaN/Inf å…± {before_fill} æ¡å·²æ›¿æ¢ä¸º 0")
        
        # ä» df ä¸­æ·»åŠ å­—æ®µï¼ˆåŸºäº (sku_id, category_id) ç»„åˆé”®åŒ¹é…ï¼‰
        # ç‰¹åˆ«æ³¨æ„ï¼šcomments_number ä½¿ç”¨æ€»è¯„è®ºæ•°ï¼Œè€Œä¸æ˜¯è¢«ä¿®æ”¹åçš„å€¼
        merge_fields = ['comments_number', 'comments_number_diff', 'days_diff', 'dt', 'next_week_dt']
        df['match_key'] = df['sku_id'] + '_' + df['category_id'].astype(str)
        output_df['match_key'] = output_df['sku_id'] + '_' + output_df['category_id'].astype(str)
        
        for field in merge_fields:
            if field in df.columns:
                # åˆ›å»ºæ˜ å°„å­—å…¸ï¼š(sku_id, category_id) -> field value
                field_map = dict(zip(df['match_key'], df[field]))
                # ä½¿ç”¨ç»„åˆé”®ä» df ä¸­æŸ¥æ‰¾å¯¹åº”çš„å€¼
                output_df[field] = output_df['match_key'].map(field_map)
                # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼å¹¶å¤„ç†
                if output_df[field].isna().any():
                    missing_count = output_df[field].isna().sum()
                    # æ•°å€¼å‹å­—æ®µï¼ˆBIGINT/DOUBLEï¼‰å¡«å……ä¸º 0ï¼Œå­—ç¬¦ä¸²å‹å¡«å……ä¸ºç©ºå­—ç¬¦ä¸²
                    if field in ['comments_number', 'comments_number_diff', 'days_diff']:
                        output_df[field] = output_df[field].fillna(0)
                        logging.warning(f"âš ï¸ è­¦å‘Šï¼š{field} å­—æ®µæœ‰ {missing_count} ä¸ªç¼ºå¤±å€¼ï¼Œå·²å¡«å……ä¸º 0")
                    else:
                        output_df[field] = output_df[field].fillna('')
                        logging.warning(f"âš ï¸ è­¦å‘Šï¼š{field} å­—æ®µæœ‰ {missing_count} ä¸ªç¼ºå¤±å€¼ï¼Œå·²å¡«å……ä¸ºç©ºå­—ç¬¦ä¸²")
        
        # æ·»åŠ  prediction_dt å­—æ®µï¼ˆé¢„æµ‹æ—¥æœŸï¼‰
        output_df['prediction_dt'] = current_date
        
        # åˆ é™¤ä¸´æ—¶åŒ¹é…é”®
        output_df = output_df.drop(columns=['match_key'])
        
        # ç¡®ä¿åˆ—é¡ºåºå¹¶è½¬æ¢æ•°æ®ç±»å‹ï¼ˆcomments_number ç°åœ¨æ˜¯æ€»è¯„è®ºæ•°ï¼‰
        # å°† BIGINT ç±»å‹å­—æ®µè½¬æ¢ä¸ºæ•´æ•°
        bigint_fields = ['comments_number', 'comments_number_diff', 'predicted_weekly_sales']
        for field in bigint_fields:
            if field in output_df.columns:
                output_df[field] = output_df[field].round().astype(int)
        
        if USE_PARTITION_TABLE:
            # åˆ†åŒºè¡¨çš„åˆ—é¡ºåºï¼ˆnext_week_dt ä½œä¸ºåˆ†åŒºå­—æ®µåœ¨æœ€åï¼‰
            output_cols = [
                'sku_id', 'category_id', 'category_name',
                'comments_number', 'comments_score', 'predicted_weekly_sales',
                'comments_number_diff', 'days_diff', 'dt', 'prediction_dt', 'next_week_dt'
            ]
        else:
            # æ™®é€šè¡¨çš„åˆ—é¡ºåºï¼ˆåŒ…å« next_week_dtï¼‰
            output_cols = [
                'sku_id', 'category_id', 'category_name',
                'comments_number', 'comments_score', 'predicted_weekly_sales',
                'comments_number_diff', 'days_diff', 'dt', 'prediction_dt', 'next_week_dt'
            ]
        
        output_df = output_df[output_cols]
        
        # 8. å†™å…¥ä¸´æ—¶CSV
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            csv_path = f.name
            output_df.to_csv(csv_path, index=False, header=False)
        logging.info(f"âœ“ ä¸´æ—¶CSVå·²ä¿å­˜")
        
        # 9. åŠ è½½æ•°æ®åˆ°Impala
        temp_table = f'{TARGET_TABLE}_temp_{int(datetime.now().timestamp())}'
        logging.info(f"åˆ›å»ºä¸´æ—¶è¡¨: {DB}.{temp_table}")
        
        impala.execute(f"DROP TABLE IF EXISTS {DB}.{temp_table}")
        
        # ä¸´æ—¶è¡¨ä½¿ç”¨æ™®é€šè¡¨ç»“æ„ï¼ˆä¸åˆ†åŒºï¼‰ï¼Œä¾¿äºåŠ è½½CSVæ•°æ®
        if USE_PARTITION_TABLE:
            # åˆ†åŒºè¡¨æ¨¡å¼ï¼šåˆ›å»ºæ™®é€šè¡¨
            # next_week_dt ä½œä¸ºæ™®é€šå­—æ®µåŒ…å«åœ¨ä¸´æ—¶è¡¨ä¸­ï¼Œæ’å…¥æ—¶ä½œä¸ºåˆ†åŒºå€¼ä½¿ç”¨
            temp_ddl = f"""
            CREATE TABLE {DB}.{temp_table} (
                sku_id STRING,
                category_id STRING,
                category_name STRING,
                comments_number BIGINT,
                comments_score DOUBLE,
                predicted_weekly_sales BIGINT,
                comments_number_diff BIGINT,
                days_diff DOUBLE,
                dt STRING,
                prediction_dt STRING,
                next_week_dt STRING
            )
            STORED AS PARQUET
            """
            impala.execute(temp_ddl)
        else:
            # è¦†ç›–æ¨¡å¼ï¼šç»§æ‰¿ç›®æ ‡è¡¨ç»“æ„
            impala.execute(f"CREATE TABLE {DB}.{temp_table} LIKE {DB}.{TARGET_TABLE}")
        
        if hive is None:
            hive = new_hive_connector(emr=True)
        loader = new_csv_to_hive_loader(
            table=temp_table,
            filename=csv_path,
            database=DB,
            hive_connector=hive,
            impala_connector=impala,
            is_std_csv=True,
            delete_file=True,
            has_header=False
        )
        loader.execute()
        logging.info(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
        
        # 10. å¯¼å…¥ç›®æ ‡è¡¨
        logging.info("å¯¼å…¥æ•°æ®åˆ°ç›®æ ‡è¡¨...")
        if USE_PARTITION_TABLE:
            # åˆ†åŒºè¡¨æ¨¡å¼ï¼šåªæ›´æ–°å½“å‰åˆ†åŒºï¼Œä¿ç•™å…¶ä»–åˆ†åŒºå†å²æ•°æ®
            logging.info(f"ä½¿ç”¨åˆ†åŒºè¡¨æ¨¡å¼")
            
            # éªŒè¯ä¸´æ—¶è¡¨ä¸­åªæœ‰ä¸€ä¸ªåˆ†åŒºå€¼ï¼ˆå®‰å…¨æ ¡éªŒï¼‰
            partition_check_sql = f"SELECT DISTINCT next_week_dt FROM {DB}.{temp_table}"
            partition_rows = impala.fetchall(partition_check_sql)
            if not partition_rows:
                raise ValueError(f"ä¸´æ—¶è¡¨ {DB}.{temp_table} ä¸­æ²¡æœ‰æ•°æ®")
            
            partition_values = [row[0] for row in partition_rows]
            if len(partition_values) > 1:
                raise ValueError(f"âš ï¸ è­¦å‘Šï¼šä¸´æ—¶è¡¨ä¸­æœ‰å¤šä¸ªåˆ†åŒºå€¼: {partition_values}ï¼Œè¿™å¯èƒ½å¯¼è‡´æ„å¤–å†™å…¥å¤šä¸ªåˆ†åŒº")
            
            partition_value = partition_values[0]
            logging.info(f"å½“å‰æ›´æ–°åˆ†åŒº: next_week_dt={partition_value}")
            
            # éªŒè¯åˆ†åŒºå€¼æ˜¯å¦ä¸é¢„æœŸçš„å½“å‰åˆ†åŒºä¸€è‡´ï¼ˆå¯é€‰æ ¡éªŒï¼‰
            if partition_value != current_next_week_dt:
                logging.warning(f"âš ï¸ æ³¨æ„ï¼šä¸´æ—¶è¡¨ä¸­çš„åˆ†åŒºå€¼ ({partition_value}) ä¸æŸ¥è¯¢æ—¶ä½¿ç”¨çš„å½“å‰åˆ†åŒº ({current_next_week_dt}) ä¸ä¸€è‡´")
            
            # å…ˆåˆ é™¤è¯¥åˆ†åŒºï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                impala.execute(f"ALTER TABLE {DB}.{TARGET_TABLE} DROP IF EXISTS PARTITION (next_week_dt='{partition_value}')")
                logging.info(f"âœ“ å·²åˆ é™¤æ—§åˆ†åŒºï¼ˆå¦‚æœå­˜åœ¨ï¼‰")
            except Exception as e:
                logging.warning(f"åˆ é™¤åˆ†åŒºæ—¶å‡ºé”™: {e}")
            
            # ä½¿ç”¨ INSERT OVERWRITE åªè¦†ç›–å½“å‰åˆ†åŒºï¼Œä¸å½±å“å…¶ä»–åˆ†åŒº
            insert_sql = f"""
            INSERT OVERWRITE TABLE {DB}.{TARGET_TABLE} PARTITION (next_week_dt)
            SELECT 
                sku_id,
                category_id,
                category_name,
                comments_number,
                comments_score,
                predicted_weekly_sales,
                comments_number_diff,
                days_diff,
                dt,
                prediction_dt,
                next_week_dt
            FROM {DB}.{temp_table}
            """
            impala.execute(insert_sql)
            logging.info(f"âœ“ æ•°æ®å·²å¯¼å…¥åˆ°åˆ†åŒºï¼Œå…¶ä»–åˆ†åŒºå†å²æ•°æ®å·²ä¿ç•™")
        else:
            # è¦†ç›–æ¨¡å¼ï¼šæ›¿æ¢å…¨éƒ¨æ•°æ®
            impala.execute(f"INSERT OVERWRITE TABLE {DB}.{TARGET_TABLE} SELECT * FROM {DB}.{temp_table}")
            logging.info(f"âœ“ æ•°æ®å·²å¯¼å…¥ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰")
        
        # 10.1. åˆå¹¶å°æ–‡ä»¶ï¼ˆä¼˜åŒ–å­˜å‚¨ï¼‰
        logging.info("åˆå¹¶å°æ–‡ä»¶ä¼˜åŒ–å­˜å‚¨...")
        try:
            check_sql = f"SHOW FILES IN {DB}.{TARGET_TABLE}"
            files_before = impala.fetchall(check_sql)
            file_count = len(files_before) if files_before else 0
            logging.info(f"ç›®æ ‡è¡¨ç°æœ‰æ–‡ä»¶æ•°: {file_count}")
            
            if file_count > 5:
                merged_temp = f'{TARGET_TABLE}_merged_temp_{int(datetime.now().timestamp())}'
                impala.execute(f"DROP TABLE IF EXISTS {DB}.{merged_temp}")
                impala.execute(f"CREATE TABLE {DB}.{merged_temp} LIKE {DB}.{TARGET_TABLE}")
                
                if USE_PARTITION_TABLE:
                    impala.execute(f"""
                        INSERT INTO {DB}.{merged_temp} PARTITION (next_week_dt)
                        SELECT sku_id, category_id, category_name, comments_number, comments_score,
                               predicted_weekly_sales, comments_number_diff, days_diff, dt, prediction_dt, next_week_dt
                        FROM {DB}.{TARGET_TABLE}
                    """)
                else:
                    impala.execute(f"INSERT INTO {DB}.{merged_temp} SELECT * FROM {DB}.{TARGET_TABLE}")
                
                impala.execute(f"DROP TABLE IF EXISTS {DB}.{TARGET_TABLE}")
                impala.execute(f"ALTER TABLE {DB}.{merged_temp} RENAME TO {TARGET_TABLE}")
                merged_temp = None  # æ ‡è®°å·²é‡å‘½åï¼Œæ— éœ€æ¸…ç†
                
                files_after = impala.fetchall(check_sql)
                file_count_after = len(files_after) if files_after else 0
                logging.info(f"âœ“ æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œæ–‡ä»¶æ•°: {file_count} -> {file_count_after}")
            else:
                logging.info(f"âœ“ æ–‡ä»¶æ•°è¾ƒå°‘ï¼Œæ— éœ€åˆå¹¶")
        except Exception as e:
            logging.warning(f"åˆå¹¶å°æ–‡ä»¶å¤±è´¥ï¼ˆä¸å½±å“ç»“æœï¼‰: {e}")
        
        # 11. æ¸…ç†ä¸´æ—¶è¡¨
        impala.execute(f"DROP TABLE IF EXISTS {DB}.{temp_table}")
        logging.info(f"âœ“ ä¸´æ—¶è¡¨å·²åˆ é™¤")
        
        # 12. åˆ é™¤ä¸´æ—¶CSVæ–‡ä»¶ï¼ˆå¦‚æœloaderæ²¡æœ‰è‡ªåŠ¨åˆ é™¤ï¼‰
        try:
            if csv_path and os.path.exists(csv_path):
                os.remove(csv_path)
                logging.info(f"âœ“ ä¸´æ—¶CSVæ–‡ä»¶å·²åˆ é™¤")
        except Exception as e:
            logging.warning(f"åˆ é™¤ä¸´æ—¶CSVæ–‡ä»¶å¤±è´¥: {e}")
        
        # 13. ç»Ÿè®¡ä¿¡æ¯
        stats_sql = f"""
        SELECT 
            COUNT(*) as total_count,
            AVG(predicted_weekly_sales) as avg_predicted_sales,
            MIN(predicted_weekly_sales) as min_predicted_sales,
            MAX(predicted_weekly_sales) as max_predicted_sales
        FROM {DB}.{TARGET_TABLE}
        """
        stats = impala.fetchall(stats_sql)[0]
        logging.info("\n" + "=" * 60)
        logging.info("ğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡")
        logging.info("=" * 60)
        logging.info(f"æ€»è®°å½•æ•°: {stats[0]:,}")
        logging.info(f"å¹³å‡é¢„æµ‹é”€é‡: {stats[1]:.2f}")
        logging.info(f"æœ€å°é¢„æµ‹é”€é‡: {stats[2]:.2f}")
        logging.info(f"æœ€å¤§é¢„æµ‹é”€é‡: {stats[3]:.2f}")
        logging.info("=" * 60)
        
        logging.info("\nâœ… å¤„ç†å®Œæˆï¼")
        logging.info(f"ç»“æœè¡¨: {DB}.{TARGET_TABLE}")
        
    except Exception as e:
        logging.error(f"âŒ å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
        raise
        
    finally:
        # æ¸…ç†ä¸´æ—¶èµ„æº
        try:
            if impala:
                if temp_table:
                    logging.info("æ¸…ç†ä¸´æ—¶è¡¨èµ„æº...")
                    impala.execute(f"DROP TABLE IF EXISTS {DB}.{temp_table}")
                    logging.info(f"âœ“ ä¸´æ—¶è¡¨å·²æ¸…ç†: {temp_table}")
                
                # æ¸…ç† merged_tempï¼ˆå¦‚æœå­˜åœ¨ä¸”æœªè¢«é‡å‘½åï¼‰
                if merged_temp:
                    logging.info("æ¸…ç†æ–‡ä»¶åˆå¹¶ä¸´æ—¶è¡¨...")
                    impala.execute(f"DROP TABLE IF EXISTS {DB}.{merged_temp}")
                    logging.info(f"âœ“ æ–‡ä»¶åˆå¹¶ä¸´æ—¶è¡¨å·²æ¸…ç†: {merged_temp}")
        except Exception as e:
            logging.warning(f"æ¸…ç†ä¸´æ—¶è¡¨å¤±è´¥: {e}")
        
        try:
            if csv_path and os.path.exists(csv_path):
                os.remove(csv_path)
                logging.info(f"âœ“ ä¸´æ—¶CSVå·²æ¸…ç†")
        except Exception as e:
            logging.warning(f"æ¸…ç†ä¸´æ—¶CSVå¤±è´¥: {e}")

if __name__ == '__main__':
    logging.info("=" * 60)
    logging.info("JDå‘¨åº¦é”€é‡é¢„æµ‹ - å¼€å§‹æ‰§è¡Œ (V2ç‰ˆæœ¬)")
    logging.info("=" * 60)
    
    main()

